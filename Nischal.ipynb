{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nischalon10/NLP_HW1/blob/main/Nischal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mathematical Formulation for IMDB Text Classification using an MLP\n",
        "# Instructor: Dr. Ankur Mali\n",
        "# University of South Florida (Spring 2025)\n",
        "\n",
        "This document describes the mathematical framework for processing IMDB text data using a character-level bag-of-characters representation, passing it through a multi-layer perceptron (MLP), and training the model via gradient descent. The evaluation metrics include loss, accuracy, precision, and recall.\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Tokenization and Input Representation\n",
        "\n",
        "Given a raw text review \\( T \\), we first tokenize it at the character level. Let \\( V \\) be the vocabulary (i.e., the set of unique characters) extracted from the training data with size \\( |V| = d \\).\n",
        "\n",
        "For each text review \\( T \\), we construct a binary bag-of-characters vector \\( x \\in \\{0,1\\}^d \\) such that:\n",
        "\n",
        "$$\n",
        "x_j =\n",
        "\\begin{cases}\n",
        "1, & \\text{if the } j\\text{-th character in } V \\text{ appears in } T, \\\\\n",
        "0, & \\text{otherwise.}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Thus, each review is represented as:\n",
        "\n",
        "$$\n",
        "x = \\mathrm{BOW}(T) \\in \\mathbb{R}^d.\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 2. MLP Model\n",
        "\n",
        "The MLP we consider has the following structure:\n",
        "- **Input layer:** Receives $$( x \\in \\mathbb{R}^d )$$.\n",
        "- **Hidden Layer 1:** With $h_1$ or $z_1$ (Post-activation) neurons.\n",
        "- **Hidden Layer 2:** With \\( h_2 \\) neurons.\n",
        "- **Output Layer:** With \\( c \\) neurons (for \\( c = 2 \\) classes in binary classification).\n",
        "\n",
        "### 2.1. Model Parameters\n",
        "\n",
        "- **First Hidden Layer:**\n",
        "  - Weight matrix: $$(W^{(1)} \\in \\mathbb{R}^{d \\times h_1} )$$\n",
        "  - Bias vector: $$( b^{(1)} \\in \\mathbb{R}^{h_1} )$$\n",
        "\n",
        "- **Second Hidden Layer:**\n",
        "  - Weight matrix: $$( W^{(2)} \\in \\mathbb{R}^{h_1 \\times h_2} )$$\n",
        "  - Bias vector: $$( b^{(2)} \\in \\mathbb{R}^{h_2} )$$\n",
        "\n",
        "- **Output Layer:**\n",
        "  - Weight matrix: $$( W^{(3)} \\in \\mathbb{R}^{h_2 \\times c} )$$\n",
        "  - Bias vector: $$( b^{(3)} \\in \\mathbb{R}^{c} )$$\n",
        "\n",
        "> **Note:** In the original code, a third hidden layer size (\\( h_3 \\)) is provided as a parameter but is not used in the forward computation. Here, the model uses two hidden layers. You can add any N layers, to this pipeline, remember to modify the pipeline accordingly.\n",
        "\n",
        "### 2.2. Forward Pass\n",
        "\n",
        "For an input vector \\( x \\), the forward propagation through the network is as follows:\n",
        "\n",
        "1. **First Hidden Layer:**\n",
        "\n",
        "   $$\n",
        "   h^{(1)} = \\text{ReLU}\\Big( x\\, W^{(1)} + b^{(1)} \\Big)\n",
        "   $$\n",
        "\n",
        "2. **Second Hidden Layer:**\n",
        "\n",
        "   $$\n",
        "   h^{(2)} = \\text{ReLU}\\Big( h^{(1)}\\, W^{(2)} + b^{(2)} \\Big)\n",
        "   $$\n",
        "\n",
        "3. **Output Layer (Logits):**\n",
        "\n",
        "   $$\n",
        "   z = h^{(2)}\\, W^{(3)} + b^{(3)}\n",
        "   $$\n",
        "\n",
        "The logits \\( z \\) are then converted to class probabilities using the softmax function:\n",
        "\n",
        "$$\n",
        "\\hat{y} = \\text{softmax}(z) = \\frac{\\exp(z)}{\\sum_{j=1}^{c} \\exp(z_j)}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Loss Function\n",
        "\n",
        "We use the **Categorical Cross Entropy Loss** (with logits) for training. For a single sample with true one-hot label \\( y \\) and predicted probabilities \\( \\hat{y} \\), the loss is:\n",
        "\n",
        "$$\n",
        "L(y, \\hat{y}) = -\\sum_{j=1}^{c} y_j \\log(\\hat{y}_j)\n",
        "$$\n",
        "\n",
        "For a batch of \\( N \\) samples, the average loss is computed as:\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{N} \\sum_{i=1}^{N} L(y^{(i)}, \\hat{y}^{(i)})\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Training via Gradient Descent\n",
        "\n",
        "The goal is to minimize the loss \\( \\mathcal{L} \\) with respect to the model parameters:\n",
        "\n",
        "$$\n",
        "\\Theta = \\{ W^{(1)},\\, b^{(1)},\\, W^{(2)},\\, b^{(2)},\\, W^{(3)},\\, b^{(3)} \\}\n",
        "$$\n",
        "\n",
        "Using gradient descent (or an adaptive method like Adam), each parameter \\( \\theta \\in \\Theta \\) is updated as:\n",
        "\n",
        "$$\n",
        "\\theta \\leftarrow \\theta - \\eta\\, \\nabla_\\theta L\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $\\eta$ is the learning rate.\n",
        "- $\\nabla_\\theta L $ denotes the gradient of the loss with respect to $\\theta $.\n",
        "\n",
        "Backpropagation is used to compute these gradients efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Evaluation Metrics\n",
        "\n",
        "In addition to monitoring the loss during training, we evaluate the model performance using:\n",
        "\n",
        "- **Accuracy:**\n",
        "\n",
        "  $$\n",
        "  \\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}\n",
        "  $$\n",
        "\n",
        "- **Precision:**\n",
        "\n",
        "  $$\n",
        "  \\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
        "  $$\n",
        "\n",
        "- **Recall:**\n",
        "\n",
        "  $$\n",
        "  \\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
        "  $$\n",
        "\n",
        "These metrics are computed on the validation and test sets to assess the model’s generalization performance.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Summary of the Pipeline\n",
        "\n",
        "1. **Tokenization:**  \n",
        "   Each review \\( T \\) is tokenized at the character level and converted into a binary vector $$x \\in \\{0,1\\}^d$$ representing the presence of each character in the vocabulary \\( V \\).\n",
        "\n",
        "2. **MLP Forward Propagation:**  \n",
        "   The input vector \\( x \\) is propagated through the MLP:\n",
        "   - First hidden layer: $$ h^{(1)} = \\text{ReLU}\\big( x\\, W^{(1)} + b^{(1)} \\big) $$\n",
        "   - Second hidden layer: $$ h^{(2)} = \\text{ReLU}\\big( h^{(1)}\\, W^{(2)} + b^{(2)} \\big) $$\n",
        "   - Output layer: $$ z = h^{(2)}\\, W^{(3)} + b^{(3)} $$\n",
        "   - Softmax conversion: $$ \\hat{y} = \\text{softmax}(z) $$\n",
        "\n",
        "3. **Loss Computation:**  \n",
        "   The categorical cross entropy loss L is computed using the true labels and the predicted probabilities.\n",
        "\n",
        "4. **Training:**  \n",
        "   The model parameters $\\Theta$ are updated using gradient descent (or Adam), where:\n",
        "\n",
        "   $$\n",
        "   \\theta \\leftarrow \\theta - \\eta\\, \\nabla_\\theta L\n",
        "   $$\n",
        "\n",
        "5. **Evaluation:**  \n",
        "   After training, the model is evaluated on the validation and test sets using the loss, accuracy, precision, and recall metrics.\n",
        "\n",
        "---\n",
        "\n",
        "This formulation captures the entire process—from transforming raw text into a numeric representation, through the forward and backward passes of an MLP, to the training and evaluation of the system. Shorter version of your slides :)\n"
      ],
      "metadata": {
        "id": "W_It9C2TJ05K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP on IMDB Dataset"
      ],
      "metadata": {
        "id": "fNGpqID8UZaW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        size_input,\n",
        "        size_output,\n",
        "        size_hidden1,\n",
        "        size_hidden2=None,\n",
        "        size_hidden3=None,\n",
        "        num_layers=2,\n",
        "        activation=\"relu\",\n",
        "        optimizer=\"Adam\",\n",
        "        learning_rate=0.001,\n",
        "        device=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int or None, size of the 2nd hidden layer (if applicable)\n",
        "        size_hidden3: int or None, size of the 3rd hidden layer (if applicable)\n",
        "        size_output: int, size of output layer\n",
        "        num_layers: int, number of hidden layers (1, 2, or 3)\n",
        "        activation: str, activation function ('relu', 'tanh', 'leaky_relu')\n",
        "        optimizer: str, optimizer ('Adam', 'SGD', 'RMSprop')\n",
        "        learning_rate: float, learning rate for optimization\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2 if num_layers > 1 else None\n",
        "        self.size_hidden3 = size_hidden3 if num_layers > 2 else None\n",
        "        self.size_output = size_output\n",
        "        self.num_layers = num_layers\n",
        "        self.device = device\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        activation_functions = {\n",
        "            \"relu\": tf.nn.relu,\n",
        "            \"tanh\": tf.nn.tanh,\n",
        "            \"leaky_relu\": tf.nn.leaky_relu,\n",
        "        }\n",
        "        self.activation = activation_functions.get(activation, tf.nn.relu)\n",
        "\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        if self.num_layers > 1:\n",
        "            self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "            self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        if self.num_layers > 2:\n",
        "            self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3], stddev=0.1))\n",
        "            self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "\n",
        "        self.W_out = tf.Variable(tf.random.normal([\n",
        "            self.size_hidden3 if self.num_layers == 3 else self.size_hidden2 if self.num_layers == 2 else self.size_hidden1,\n",
        "            self.size_output\n",
        "        ], stddev=0.1))\n",
        "        self.b_out = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        self.variables = [self.W1, self.b1]\n",
        "        if self.num_layers > 1:\n",
        "            self.variables.extend([self.W2, self.b2])\n",
        "        if self.num_layers > 2:\n",
        "            self.variables.extend([self.W3, self.b3])\n",
        "        self.variables.extend([self.W_out, self.b_out])\n",
        "\n",
        "        optimizers = {\n",
        "            \"Adam\": tf.keras.optimizers.Adam,\n",
        "            \"SGD\": tf.keras.optimizers.SGD,\n",
        "            \"RMSprop\": tf.keras.optimizers.RMSprop,\n",
        "        }\n",
        "        self.optimizer = optimizers.get(optimizer)(learning_rate=self.learning_rate)\n",
        "\n",
        "    def forward(self, X):\n",
        "        if self.device is not None:\n",
        "            with tf.device(\"gpu:0\" if self.device == \"gpu\" else \"cpu\"):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred: Tensor of shape (batch_size, size_output)\n",
        "        y_true: Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass: compute gradients of the loss with respect to the variables.\n",
        "        \"\"\"\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        h1 = self.activation(tf.matmul(X_tf, self.W1) + self.b1)\n",
        "\n",
        "        if self.num_layers > 1:\n",
        "            h2 = self.activation(tf.matmul(h1, self.W2) + self.b2)\n",
        "        if self.num_layers > 2:\n",
        "            h3 = self.activation(tf.matmul(h2, self.W3) + self.b3)\n",
        "\n",
        "        output = tf.matmul(h3 if self.num_layers == 3 else h2 if self.num_layers == 2 else h1, self.W_out) + self.b_out\n",
        "        return output\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Word-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=num_words, char_level=True, lower=True\n",
        "    )\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode=\"binary\")\n",
        "    return matrix\n",
        "\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    \"imdb_reviews\", split=[\"train\", \"test\"], as_supervised=True, with_info=True\n",
        ")\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode(\"utf-8\"))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode(\"utf-8\"))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(\n",
        "    f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\"\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val = texts_to_bow(tokenizer, val_texts)\n",
        "X_test = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val = one_hot_encode(val_labels)\n",
        "y_test = one_hot_encode(test_labels)\n",
        "\n",
        "\n",
        "print(f\"NUM_LAYERS, HIDDEN_SIZE, ACTIVATION, OPTIMIZER, LEARNING, BATCH_SIZE, Test Accuracy\\n\")\n",
        "for BATCH_SIZE in [128, 64, 32]:\n",
        "    for NUM_LAYERS in [2, 3, 1]:\n",
        "        for HIDDEN_SIZE in [128, 256, 512]:\n",
        "            for ACTIVATION in [\"relu\", \"tanh\", \"leaky_relu\"]:\n",
        "                for OPTIMIZER in [\"Adam\", \"SGD\", \"RMSprop\"]:\n",
        "                    for LEARNING in [0.001, 0.0005, 0.0001]:\n",
        "                        # -------------------------------\n",
        "                        # Model Setup\n",
        "                        # -------------------------------\n",
        "                        # The input size is determined by the dimension of the bag-of-characters vector.\n",
        "                        size_input = X_train.shape[1]\n",
        "                        size_hidden1 = HIDDEN_SIZE\n",
        "                        size_hidden2 = HIDDEN_SIZE\n",
        "                        size_hidden3 = HIDDEN_SIZE\n",
        "                        size_output = 2  # Binary classification.\n",
        "                        batch_size = BATCH_SIZE\n",
        "                        epochs = 10\n",
        "                        num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "                        # Instantiate the MLP model.\n",
        "                        # model = MLP(\n",
        "                        #     size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None\n",
        "                        # )\n",
        "                        model = MLP(\n",
        "                            size_input=size_input,\n",
        "                            size_output=size_output,\n",
        "                            size_hidden1=size_hidden1,\n",
        "                            size_hidden2=size_hidden2,\n",
        "                            size_hidden3=size_hidden3,\n",
        "                            num_layers=NUM_LAYERS,\n",
        "                            activation=ACTIVATION,\n",
        "                            optimizer=OPTIMIZER,\n",
        "                            learning_rate=LEARNING,\n",
        "                            device=None,\n",
        "                        )\n",
        "\n",
        "                        # Define the optimizer.\n",
        "                        # optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "                        optimizers = {\n",
        "                            \"Adam\": tf.keras.optimizers.Adam,\n",
        "                            \"SGD\": tf.keras.optimizers.SGD,\n",
        "                            \"RMSprop\": tf.keras.optimizers.RMSprop,\n",
        "                        }\n",
        "                        optimizer = optimizers.get(OPTIMIZER)(learning_rate=LEARNING)\n",
        "\n",
        "                        for epoch in range(epochs):\n",
        "                            # Shuffle training data at the start of each epoch.\n",
        "                            indices = np.arange(X_train.shape[0])\n",
        "                            np.random.shuffle(indices)\n",
        "                            X_train = X_train[indices]\n",
        "                            y_train = y_train[indices]\n",
        "\n",
        "                            epoch_loss = 0\n",
        "                            for i in range(num_batches):\n",
        "                                start = i * batch_size\n",
        "                                end = min((i + 1) * batch_size, X_train.shape[0])\n",
        "                                X_batch = X_train[start:end]\n",
        "                                y_batch = y_train[start:end]\n",
        "                                predictions = model.forward(X_batch)\n",
        "                                loss_value = model.loss(predictions, y_batch)\n",
        "                                grads = model.backward(X_batch, y_batch)\n",
        "                                optimizer.apply_gradients(zip(grads, model.variables))\n",
        "                                epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "                            epoch_loss /= X_train.shape[0]\n",
        "\n",
        "                            # Evaluate on validation set.\n",
        "                            val_logits = model.forward(X_val)\n",
        "                            val_loss = model.loss(val_logits, y_val).numpy()\n",
        "                            val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "                            true_val = np.argmax(y_val, axis=1)\n",
        "                            accuracy = np.mean(val_preds == true_val)\n",
        "                            precision = precision_score(true_val, val_preds)\n",
        "                            recall = recall_score(true_val, val_preds)\n",
        "\n",
        "                            # print(\n",
        "                            #     f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "                            #     f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\"\n",
        "                            # )\n",
        "\n",
        "                            # # -------------------------------\n",
        "                            # # Final Evaluation on Test Set\n",
        "                            # # -------------------------------\n",
        "                            # print(\"\\nEvaluating on test set...\")\n",
        "                        test_logits = model.forward(X_test)\n",
        "                        test_loss = model.loss(test_logits, y_test).numpy()\n",
        "                        test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "                        true_test = np.argmax(y_test, axis=1)\n",
        "                        test_accuracy = np.mean(test_preds == true_test)\n",
        "                        test_precision = precision_score(true_test, test_preds)\n",
        "                        test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "                            # print(\n",
        "                            #     f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "                            #     f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\"\n",
        "                            # )\n",
        "\n",
        "                        print(f\"{NUM_LAYERS},{HIDDEN_SIZE},{ACTIVATION},{OPTIMIZER},{LEARNING},{BATCH_SIZE},{test_accuracy:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qb0sCnSFsHJl",
        "outputId": "8ff18ab9-2339-4a03-ead8-a5d080107a66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 80169\n",
            "NUM_LAYERS, HIDDEN_SIZE, ACTIVATION, OPTIMIZER, LEARNING, BATCH_SIZE, Test Accuracy\n",
            "\n",
            "1,128,relu,Adam,0.001,128,0.8454\n",
            "\n",
            "1,128,relu,Adam,0.0005,128,0.8507\n",
            "\n",
            "1,128,relu,Adam,0.0001,128,0.8559\n",
            "\n",
            "1,128,relu,SGD,0.001,128,0.6173\n",
            "\n",
            "1,128,relu,SGD,0.0005,128,0.5524\n",
            "\n",
            "1,128,relu,SGD,0.0001,128,0.4694\n",
            "\n",
            "1,128,relu,RMSprop,0.001,128,0.8396\n",
            "\n",
            "1,128,relu,RMSprop,0.0005,128,0.8494\n",
            "\n",
            "1,128,relu,RMSprop,0.0001,128,0.8535\n",
            "\n",
            "1,128,tanh,Adam,0.001,128,0.8504\n",
            "\n",
            "1,128,tanh,Adam,0.0005,128,0.8532\n",
            "\n",
            "1,128,tanh,Adam,0.0001,128,0.8577\n",
            "\n",
            "1,128,tanh,SGD,0.001,128,0.6142\n",
            "\n",
            "1,128,tanh,SGD,0.0005,128,0.5560\n",
            "\n",
            "1,128,tanh,SGD,0.0001,128,0.5063\n",
            "\n",
            "1,128,tanh,RMSprop,0.001,128,0.8534\n",
            "\n",
            "1,128,tanh,RMSprop,0.0005,128,0.8559\n",
            "\n",
            "1,128,tanh,RMSprop,0.0001,128,0.8577\n",
            "\n",
            "1,128,leaky_relu,Adam,0.001,128,0.8482\n",
            "\n",
            "1,128,leaky_relu,Adam,0.0005,128,0.8522\n",
            "\n",
            "1,128,leaky_relu,Adam,0.0001,128,0.8580\n",
            "\n",
            "1,128,leaky_relu,SGD,0.001,128,0.6458\n",
            "\n",
            "1,128,leaky_relu,SGD,0.0005,128,0.5880\n",
            "\n",
            "1,128,leaky_relu,SGD,0.0001,128,0.5043\n",
            "\n",
            "1,128,leaky_relu,RMSprop,0.001,128,0.8411\n",
            "\n",
            "1,128,leaky_relu,RMSprop,0.0005,128,0.8546\n",
            "\n",
            "1,128,leaky_relu,RMSprop,0.0001,128,0.8578\n",
            "\n",
            "1,256,relu,Adam,0.001,128,0.8523\n",
            "\n",
            "1,256,relu,Adam,0.0005,128,0.8532\n",
            "\n",
            "1,256,relu,Adam,0.0001,128,0.8527\n",
            "\n",
            "1,256,relu,SGD,0.001,128,0.6205\n",
            "\n",
            "1,256,relu,SGD,0.0005,128,0.5733\n",
            "\n",
            "1,256,relu,SGD,0.0001,128,0.5058\n",
            "\n",
            "1,256,relu,RMSprop,0.001,128,0.8486\n",
            "\n",
            "1,256,relu,RMSprop,0.0005,128,0.8457\n",
            "\n",
            "1,256,relu,RMSprop,0.0001,128,0.8546\n",
            "\n",
            "1,256,tanh,Adam,0.001,128,0.8499\n",
            "\n",
            "1,256,tanh,Adam,0.0005,128,0.8515\n",
            "\n",
            "1,256,tanh,Adam,0.0001,128,0.8548\n",
            "\n",
            "1,256,tanh,SGD,0.001,128,0.6591\n",
            "\n",
            "1,256,tanh,SGD,0.0005,128,0.5824\n",
            "\n",
            "1,256,tanh,SGD,0.0001,128,0.5352\n",
            "\n",
            "1,256,tanh,RMSprop,0.001,128,0.8545\n",
            "\n",
            "1,256,tanh,RMSprop,0.0005,128,0.8546\n",
            "\n",
            "1,256,tanh,RMSprop,0.0001,128,0.8581\n",
            "\n",
            "1,256,leaky_relu,Adam,0.001,128,0.8495\n",
            "\n",
            "1,256,leaky_relu,Adam,0.0005,128,0.8501\n",
            "\n",
            "1,256,leaky_relu,Adam,0.0001,128,0.8546\n",
            "\n",
            "1,256,leaky_relu,SGD,0.001,128,0.6663\n",
            "\n",
            "1,256,leaky_relu,SGD,0.0005,128,0.5807\n",
            "\n",
            "1,256,leaky_relu,SGD,0.0001,128,0.5448\n",
            "\n",
            "1,256,leaky_relu,RMSprop,0.001,128,0.8493\n",
            "\n",
            "1,256,leaky_relu,RMSprop,0.0005,128,0.8502\n",
            "\n",
            "1,256,leaky_relu,RMSprop,0.0001,128,0.8562\n",
            "\n",
            "1,512,relu,Adam,0.001,128,0.8502\n",
            "\n",
            "1,512,relu,Adam,0.0005,128,0.8512\n",
            "\n",
            "1,512,relu,Adam,0.0001,128,0.8493\n",
            "\n",
            "1,512,relu,SGD,0.001,128,0.6784\n",
            "\n",
            "1,512,relu,SGD,0.0005,128,0.5925\n",
            "\n",
            "1,512,relu,SGD,0.0001,128,0.5256\n",
            "\n",
            "1,512,relu,RMSprop,0.001,128,0.8482\n",
            "\n",
            "1,512,relu,RMSprop,0.0005,128,0.8420\n",
            "\n",
            "1,512,relu,RMSprop,0.0001,128,0.8518\n",
            "\n",
            "1,512,tanh,Adam,0.001,128,0.8479\n",
            "\n",
            "1,512,tanh,Adam,0.0005,128,0.8486\n",
            "\n",
            "1,512,tanh,Adam,0.0001,128,0.8550\n",
            "\n",
            "1,512,tanh,SGD,0.001,128,0.6933\n",
            "\n",
            "1,512,tanh,SGD,0.0005,128,0.6355\n",
            "\n",
            "1,512,tanh,SGD,0.0001,128,0.5207\n",
            "\n",
            "1,512,tanh,RMSprop,0.001,128,0.8511\n",
            "\n",
            "1,512,tanh,RMSprop,0.0005,128,0.8536\n",
            "\n",
            "1,512,tanh,RMSprop,0.0001,128,0.8551\n",
            "\n",
            "1,512,leaky_relu,Adam,0.001,128,0.8539\n",
            "\n",
            "1,512,leaky_relu,Adam,0.0005,128,0.8513\n",
            "\n",
            "1,512,leaky_relu,Adam,0.0001,128,0.8544\n",
            "\n",
            "1,512,leaky_relu,SGD,0.001,128,0.6788\n",
            "\n",
            "1,512,leaky_relu,SGD,0.0005,128,0.6252\n",
            "\n",
            "1,512,leaky_relu,SGD,0.0001,128,0.5267\n",
            "\n",
            "1,512,leaky_relu,RMSprop,0.001,128,0.8080\n",
            "\n",
            "1,512,leaky_relu,RMSprop,0.0005,128,0.8464\n",
            "\n",
            "1,512,leaky_relu,RMSprop,0.0001,128,0.8494\n",
            "\n",
            "2,128,relu,Adam,0.001,128,0.8355\n",
            "\n",
            "2,128,relu,Adam,0.0005,128,0.8336\n",
            "\n",
            "2,128,relu,Adam,0.0001,128,0.8511\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random MLP on IMDB Dataset"
      ],
      "metadata": {
        "id": "Zpy6iEakUWAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score\n",
        "\n",
        "tf.random.set_seed(1221)\n",
        "np.random.seed(1221)\n",
        "\n",
        "\n",
        "# ---------------------------\n",
        "# Hyperparameters\n",
        "BATCH_SIZE = 128\n",
        "NUM_LAYERS = 1\n",
        "HIDDEN_SIZE = 256\n",
        "ACTIVATION = \"tanh\"\n",
        "OPTIMIZER = \"RMSprop\"\n",
        "LEARNING = 0.0001\n",
        "# ---------------------------\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Original MLP Class Definition\n",
        "# -------------------------------\n",
        "class MLP_rnd(object):\n",
        "    def __init__(\n",
        "        self,\n",
        "        size_input,\n",
        "        size_output,\n",
        "        size_hidden1,\n",
        "        size_hidden2=None,\n",
        "        size_hidden3=None,\n",
        "        num_layers=2,\n",
        "        activation=\"relu\",\n",
        "        device=None,\n",
        "    ):\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2 if num_layers > 1 else None\n",
        "        self.size_hidden3 = size_hidden3 if num_layers > 2 else None\n",
        "        self.size_output = size_output\n",
        "        self.num_layers = num_layers\n",
        "        self.device = device\n",
        "\n",
        "        activation_functions = {\n",
        "            \"relu\": tf.nn.relu,\n",
        "            \"tanh\": tf.nn.tanh,\n",
        "            \"leaky_relu\": tf.nn.leaky_relu,\n",
        "        }\n",
        "        self.activation = activation_functions.get(activation, tf.nn.relu)\n",
        "\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        if self.num_layers > 1:\n",
        "            self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "            self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        if self.num_layers > 2:\n",
        "            self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3], stddev=0.1))\n",
        "            self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "\n",
        "        self.W_out = tf.Variable(tf.random.normal([\n",
        "            self.size_hidden3 if self.num_layers == 3 else self.size_hidden2 if self.num_layers == 2 else self.size_hidden1,\n",
        "            self.size_output\n",
        "        ], stddev=0.1))\n",
        "        self.b_out = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        self.variables = [self.W_out, self.b_out]\n",
        "        if self.num_layers > 2:\n",
        "            self.variables.extend([self.W3, self.b3])\n",
        "        if self.num_layers > 1:\n",
        "            self.variables.extend([self.W2, self.b2])\n",
        "        self.variables.extend([self.W1, self.b1])\n",
        "\n",
        "    def forward(self, X):\n",
        "        if self.device is not None:\n",
        "            with tf.device(\"gpu:0\" if self.device == \"gpu\" else \"cpu\"):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        with tf.GradientTape() as tape:\n",
        "            predicted = self.forward(X_train)\n",
        "            current_loss = self.loss(predicted, y_train)\n",
        "        grads = tape.gradient(current_loss, self.variables)\n",
        "        return grads\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        h1 = self.activation(tf.matmul(X_tf, self.W1) + self.b1)\n",
        "\n",
        "        if self.num_layers > 1:\n",
        "            h2 = self.activation(tf.matmul(h1, self.W2) + self.b2)\n",
        "        if self.num_layers > 2:\n",
        "            h3 = self.activation(tf.matmul(h2, self.W3) + self.b3)\n",
        "\n",
        "        output = tf.matmul(h3 if self.num_layers == 3 else h2 if self.num_layers == 2 else h1, self.W_out) + self.b_out\n",
        "        return output\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=1000):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "        num_words=num_words, char_level=False, lower=True\n",
        "    )\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode=\"binary\")\n",
        "    return matrix\n",
        "\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load(\n",
        "    \"imdb_reviews\", split=[\"train\", \"test\"], as_supervised=True, with_info=True\n",
        ")\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode(\"utf-8\"))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode(\"utf-8\"))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(\n",
        "    f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\"\n",
        ")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val = texts_to_bow(tokenizer, val_texts)\n",
        "X_test = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val = one_hot_encode(val_labels)\n",
        "y_test = one_hot_encode(test_labels)\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = HIDDEN_SIZE\n",
        "size_hidden2 = HIDDEN_SIZE\n",
        "size_hidden3 = HIDDEN_SIZE\n",
        "size_output = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP_rnd(\n",
        "    size_input=size_input,\n",
        "    size_output=size_output,\n",
        "    size_hidden1=size_hidden1,\n",
        "    size_hidden2=size_hidden2,\n",
        "    size_hidden3=size_hidden3,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    activation=ACTIVATION,\n",
        "    device=None,\n",
        ")\n",
        "\n",
        "optimizers = {\n",
        "    \"Adam\": tf.keras.optimizers.Adam,\n",
        "    \"SGD\": tf.keras.optimizers.SGD,\n",
        "    \"RMSprop\": tf.keras.optimizers.RMSprop,\n",
        "}\n",
        "optimizer = optimizers.get(OPTIMIZER)(learning_rate=LEARNING)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = BATCH_SIZE\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i + 1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "        f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\"\n",
        "    )\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(\n",
        "    f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "    f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0MseUvkuqd0",
        "outputId": "743bfee2-02cb-4c85-8b53-a286f50d5989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6879 | Val Loss: 0.6852 | Accuracy: 0.5636 | Precision: 0.5433 | Recall: 0.6258\n",
            "Epoch 02 | Training Loss: 0.6828 | Val Loss: 0.6820 | Accuracy: 0.5704 | Precision: 0.5454 | Recall: 0.6840\n",
            "Epoch 03 | Training Loss: 0.6796 | Val Loss: 0.6796 | Accuracy: 0.5764 | Precision: 0.5512 | Recall: 0.6795\n",
            "Epoch 04 | Training Loss: 0.6775 | Val Loss: 0.6771 | Accuracy: 0.5802 | Precision: 0.5641 | Recall: 0.5903\n",
            "Epoch 05 | Training Loss: 0.6759 | Val Loss: 0.6767 | Accuracy: 0.5822 | Precision: 0.5592 | Recall: 0.6531\n",
            "Epoch 06 | Training Loss: 0.6747 | Val Loss: 0.6756 | Accuracy: 0.5852 | Precision: 0.5651 | Recall: 0.6267\n",
            "Epoch 07 | Training Loss: 0.6739 | Val Loss: 0.6749 | Accuracy: 0.5866 | Precision: 0.5676 | Recall: 0.6180\n",
            "Epoch 08 | Training Loss: 0.6731 | Val Loss: 0.6746 | Accuracy: 0.5860 | Precision: 0.5660 | Recall: 0.6258\n",
            "Epoch 09 | Training Loss: 0.6725 | Val Loss: 0.6744 | Accuracy: 0.5850 | Precision: 0.5643 | Recall: 0.6320\n",
            "Epoch 10 | Training Loss: 0.6722 | Val Loss: 0.6734 | Accuracy: 0.5840 | Precision: 0.5710 | Recall: 0.5705\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6714 | Test Accuracy: 0.5888 | Test Precision: 0.5906 | Test Recall: 0.5786\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP with feedback alignment on IMDB Dataset"
      ],
      "metadata": {
        "id": "p-wETEHOUGuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP_FA(object):\n",
        "    def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "        \"\"\"\n",
        "        size_input: int, size of input layer\n",
        "        size_hidden1: int, size of the 1st hidden layer\n",
        "        size_hidden2: int, size of the 2nd hidden layer\n",
        "        size_hidden3: int, size of the 3rd hidden layer (Note: Not used in compute_output in this example)\n",
        "        size_output: int, size of output layer\n",
        "        device: str or None, either 'cpu' or 'gpu' or None.\n",
        "        \"\"\"\n",
        "        self.size_input = size_input\n",
        "        self.size_hidden1 = size_hidden1\n",
        "        self.size_hidden2 = size_hidden2\n",
        "        self.size_hidden3 = size_hidden3  # (Currently not used)\n",
        "        self.size_output = size_output\n",
        "        self.device = device\n",
        "\n",
        "        # Initialize weights and biases for first hidden layer\n",
        "        self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1], stddev=0.1))\n",
        "        self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1]))\n",
        "\n",
        "        # Initialize weights and biases for second hidden layer\n",
        "        self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2], stddev=0.1))\n",
        "        self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "\n",
        "        # Initialize weights and biases for output layer\n",
        "        self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_output], stddev=0.1))\n",
        "        self.b3 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "\n",
        "        # Create fixed random feedback matrices for feedback alignment:\n",
        "        # B3: used to propagate the error from the output layer to the second hidden layer.\n",
        "        # It replaces the use of W3^T. Its shape is (size_output, size_hidden2).\n",
        "        self.B3 = tf.Variable(tf.random.normal([self.size_output, self.size_hidden2]), trainable=False)\n",
        "\n",
        "        # B2: used to propagate the error from the second hidden layer to the first hidden layer.\n",
        "        # Its shape is (size_hidden2, size_hidden1).\n",
        "        self.B2 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden1]), trainable=False)\n",
        "\n",
        "        # Define variables to be updated during training\n",
        "        self.variables = [self.W1, self.W2, self.W3, self.b1, self.b2, self.b3]\n",
        "\n",
        "    def forward(self, X):\n",
        "        \"\"\"\n",
        "        Forward pass.\n",
        "        X: Tensor, inputs.\n",
        "        \"\"\"\n",
        "        if self.device is not None:\n",
        "            with tf.device('gpu:0' if self.device == 'gpu' else 'cpu'):\n",
        "                self.y = self.compute_output(X)\n",
        "        else:\n",
        "            self.y = self.compute_output(X)\n",
        "        return self.y\n",
        "\n",
        "    def loss(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        Computes the loss between predicted and true outputs.\n",
        "        y_pred - Tensor of shape (batch_size, size_output)\n",
        "        y_true - Tensor of shape (batch_size, size_output)\n",
        "        \"\"\"\n",
        "        y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "        y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "        cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "        loss_x = cce(y_true_tf, y_pred_tf)\n",
        "        return loss_x\n",
        "\n",
        "    def backward(self, X_train, y_train):\n",
        "        \"\"\"\n",
        "        Backward pass using feedback alignment.\n",
        "        Computes gradients manually using fixed random feedback matrices.\n",
        "        X_train: Input data (numpy array)\n",
        "        y_train: One-hot encoded labels (numpy array)\n",
        "        Returns: List of gradients corresponding to [dW1, dW2, dW3, db1, db2, db3]\n",
        "        \"\"\"\n",
        "        # Cast input to float32 tensor\n",
        "        X_tf = tf.cast(X_train, tf.float32)\n",
        "\n",
        "        # --- Forward Pass ---\n",
        "        # First hidden layer\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        a1 = tf.nn.relu(h1)\n",
        "        # Second hidden layer\n",
        "        h2 = tf.matmul(a1, self.W2) + self.b2\n",
        "        a2 = tf.nn.relu(h2)\n",
        "        # Output layer (logits)\n",
        "        logits = tf.matmul(a2, self.W3) + self.b3\n",
        "        # Softmax predictions\n",
        "        y_pred = tf.nn.softmax(logits)\n",
        "\n",
        "        # --- Compute Output Error ---\n",
        "        # For cross-entropy with softmax, the derivative is (y_pred - y_true)\n",
        "        delta3 = y_pred - tf.cast(y_train, tf.float32)  # shape: (batch, size_output)\n",
        "        batch_size = tf.cast(tf.shape(X_tf)[0], tf.float32)\n",
        "\n",
        "        # --- Gradients for Output Layer ---\n",
        "        dW3 = tf.matmul(tf.transpose(a2), delta3) / batch_size\n",
        "        db3 = tf.reduce_mean(delta3, axis=0, keepdims=True)\n",
        "\n",
        "        # --- Feedback Alignment for Second Hidden Layer ---\n",
        "        # Instead of delta2 = (delta3 dot W3^T) * ReLU'(h2), use a fixed random matrix B3.\n",
        "        relu_grad_h2 = tf.cast(h2 > 0, tf.float32)\n",
        "        # delta3 has shape (batch, size_output) and B3 has shape (size_output, size_hidden2)\n",
        "        delta2 = tf.matmul(delta3, self.B3) * relu_grad_h2  # shape: (batch, size_hidden2)\n",
        "\n",
        "        dW2 = tf.matmul(tf.transpose(a1), delta2) / batch_size\n",
        "        db2 = tf.reduce_mean(delta2, axis=0, keepdims=True)\n",
        "\n",
        "        # --- Feedback Alignment for First Hidden Layer ---\n",
        "        # Instead of delta1 = (delta2 dot W2^T) * ReLU'(h1), use a fixed random matrix B2.\n",
        "        relu_grad_h1 = tf.cast(h1 > 0, tf.float32)\n",
        "        # delta2 has shape (batch, size_hidden2) and B2 has shape (size_hidden2, size_hidden1)\n",
        "        delta1 = tf.matmul(delta2, self.B2) * relu_grad_h1  # shape: (batch, size_hidden1)\n",
        "\n",
        "        dW1 = tf.matmul(tf.transpose(X_tf), delta1) / batch_size\n",
        "        db1 = tf.reduce_mean(delta1, axis=0, keepdims=True)\n",
        "\n",
        "        return [dW1, dW2, dW3, db1, db2, db3]\n",
        "\n",
        "    def compute_output(self, X):\n",
        "        \"\"\"\n",
        "        Custom method to obtain output tensor during the forward pass.\n",
        "        \"\"\"\n",
        "        X_tf = tf.cast(X, dtype=tf.float32)\n",
        "        h1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "        z1 = tf.nn.relu(h1)\n",
        "        h2 = tf.matmul(z1, self.W2) + self.b2\n",
        "        z2 = tf.nn.relu(h2)\n",
        "        output = tf.matmul(z2, self.W3) + self.b3\n",
        "        return output\n",
        "\n",
        "\n",
        "# -------------------------------\n",
        "# Character-Level Tokenizer and Preprocessing Functions\n",
        "# -------------------------------\n",
        "def char_level_tokenizer(texts, num_words=None):\n",
        "    \"\"\"\n",
        "    Create and fit a character-level tokenizer.\n",
        "\n",
        "    Args:\n",
        "        texts (list of str): List of texts.\n",
        "        num_words (int or None): Maximum number of tokens to keep.\n",
        "\n",
        "    Returns:\n",
        "        tokenizer: A fitted Tokenizer instance.\n",
        "    \"\"\"\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, char_level=True, lower=True)\n",
        "    tokenizer.fit_on_texts(texts)\n",
        "    return tokenizer\n",
        "\n",
        "def texts_to_bow(tokenizer, texts):\n",
        "    \"\"\"\n",
        "    Convert texts to a bag-of-characters representation.\n",
        "\n",
        "    Args:\n",
        "        tokenizer: A fitted character-level Tokenizer.\n",
        "        texts (list of str): List of texts.\n",
        "\n",
        "    Returns:\n",
        "        Numpy array representing the binary bag-of-characters for each text.\n",
        "    \"\"\"\n",
        "    # texts_to_matrix with mode 'binary' produces a fixed-length binary vector per text.\n",
        "    matrix = tokenizer.texts_to_matrix(texts, mode='binary')\n",
        "    return matrix\n",
        "\n",
        "def one_hot_encode(labels, num_classes=2):\n",
        "    \"\"\"\n",
        "    Convert numeric labels to one-hot encoded vectors.\n",
        "    \"\"\"\n",
        "    return np.eye(num_classes)[labels]\n",
        "\n",
        "# -------------------------------\n",
        "# Load and Prepare the IMDB Dataset\n",
        "# -------------------------------\n",
        "print(\"Loading IMDB dataset...\")\n",
        "# Load the IMDB reviews dataset with the 'as_supervised' flag so that we get (text, label) pairs.\n",
        "(ds_train, ds_test), ds_info = tfds.load('imdb_reviews',\n",
        "                                           split=['train', 'test'],\n",
        "                                           as_supervised=True,\n",
        "                                           with_info=True)\n",
        "\n",
        "# Convert training dataset to lists.\n",
        "train_texts = []\n",
        "train_labels = []\n",
        "for text, label in tfds.as_numpy(ds_train):\n",
        "    # Decode byte strings to utf-8 strings.\n",
        "    train_texts.append(text.decode('utf-8'))\n",
        "    train_labels.append(label)\n",
        "train_labels = np.array(train_labels)\n",
        "\n",
        "# Create a validation set from the training data (20% for validation).\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_texts, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert test dataset to lists.\n",
        "test_texts = []\n",
        "test_labels = []\n",
        "for text, label in tfds.as_numpy(ds_test):\n",
        "    test_texts.append(text.decode('utf-8'))\n",
        "    test_labels.append(label)\n",
        "test_labels = np.array(test_labels)\n",
        "\n",
        "print(f\"Train samples: {len(train_texts)}, Validation samples: {len(val_texts)}, Test samples: {len(test_texts)}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Preprocessing: Tokenization and Vectorization\n",
        "# -------------------------------\n",
        "# Build the character-level tokenizer on the training texts.\n",
        "tokenizer = char_level_tokenizer(train_texts)\n",
        "print(\"Tokenizer vocabulary size:\", len(tokenizer.word_index) + 1)\n",
        "\n",
        "# Convert texts to bag-of-characters representation.\n",
        "X_train = texts_to_bow(tokenizer, train_texts)\n",
        "X_val   = texts_to_bow(tokenizer, val_texts)\n",
        "X_test  = texts_to_bow(tokenizer, test_texts)\n",
        "\n",
        "# Convert labels to one-hot encoding.\n",
        "y_train = one_hot_encode(train_labels)\n",
        "y_val   = one_hot_encode(val_labels)\n",
        "y_test  = one_hot_encode(test_labels)\n",
        "\n",
        "# -------------------------------\n",
        "# Model Setup\n",
        "# -------------------------------\n",
        "# The input size is determined by the dimension of the bag-of-characters vector.\n",
        "size_input = X_train.shape[1]\n",
        "# Set hidden layer sizes as desired.\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 64\n",
        "size_hidden3 = 32  # Placeholder (not used in the forward pass)\n",
        "size_output  = 2\n",
        "\n",
        "# Instantiate the MLP model.\n",
        "model = MLP_FA(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None)\n",
        "\n",
        "# Define the optimizer.\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# -------------------------------\n",
        "# Training Parameters and Loop\n",
        "# -------------------------------\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "num_batches = int(np.ceil(X_train.shape[0] / batch_size))\n",
        "\n",
        "print(\"\\nStarting training...\\n\")\n",
        "for epoch in range(epochs):\n",
        "    # Shuffle training data at the start of each epoch.\n",
        "    indices = np.arange(X_train.shape[0])\n",
        "    np.random.shuffle(indices)\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    epoch_loss = 0\n",
        "    for i in range(num_batches):\n",
        "        start = i * batch_size\n",
        "        end = min((i+1) * batch_size, X_train.shape[0])\n",
        "        X_batch = X_train[start:end]\n",
        "        y_batch = y_train[start:end]\n",
        "\n",
        "        # Compute gradients and update weights.\n",
        "        # with tf.GradientTape() as tape:\n",
        "        #     predictions = model.forward(X_batch)\n",
        "        #     loss_value = model.loss(predictions, y_batch)\n",
        "        # grads = tape.gradient(loss_value, model.variables)\n",
        "        predictions = model.forward(X_batch)\n",
        "        loss_value = model.loss(predictions, y_batch)\n",
        "        grads = model.backward(X_batch, y_batch)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        epoch_loss += loss_value.numpy() * (end - start)\n",
        "\n",
        "    epoch_loss /= X_train.shape[0]\n",
        "\n",
        "    # Evaluate on validation set.\n",
        "    val_logits = model.forward(X_val)\n",
        "    val_loss = model.loss(val_logits, y_val).numpy()\n",
        "    val_preds = np.argmax(val_logits.numpy(), axis=1)\n",
        "    true_val = np.argmax(y_val, axis=1)\n",
        "    accuracy = np.mean(val_preds == true_val)\n",
        "    precision = precision_score(true_val, val_preds)\n",
        "    recall = recall_score(true_val, val_preds)\n",
        "\n",
        "    print(f\"Epoch {epoch+1:02d} | Training Loss: {epoch_loss:.4f} | Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Accuracy: {accuracy:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Final Evaluation on Test Set\n",
        "# -------------------------------\n",
        "print(\"\\nEvaluating on test set...\")\n",
        "test_logits = model.forward(X_test)\n",
        "test_loss = model.loss(test_logits, y_test).numpy()\n",
        "test_preds = np.argmax(test_logits.numpy(), axis=1)\n",
        "true_test = np.argmax(y_test, axis=1)\n",
        "test_accuracy = np.mean(test_preds == true_test)\n",
        "test_precision = precision_score(true_test, test_preds)\n",
        "test_recall = recall_score(true_test, test_preds)\n",
        "\n",
        "print(f\"Test Loss: {test_loss:.4f} | Test Accuracy: {test_accuracy:.4f} | \"\n",
        "      f\"Test Precision: {test_precision:.4f} | Test Recall: {test_recall:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mG5AfD2DTdNy",
        "outputId": "1d403256-236f-40c0-bf0f-d192d297ba7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading IMDB dataset...\n",
            "Train samples: 20000, Validation samples: 5000, Test samples: 25000\n",
            "Tokenizer vocabulary size: 134\n",
            "\n",
            "Starting training...\n",
            "\n",
            "Epoch 01 | Training Loss: 0.6810 | Val Loss: 0.6650 | Accuracy: 0.6048 | Precision: 0.5828 | Recall: 0.6502\n",
            "Epoch 02 | Training Loss: 0.6634 | Val Loss: 0.6642 | Accuracy: 0.6060 | Precision: 0.5821 | Recall: 0.6638\n",
            "Epoch 03 | Training Loss: 0.6629 | Val Loss: 0.6626 | Accuracy: 0.6098 | Precision: 0.5844 | Recall: 0.6753\n",
            "Epoch 04 | Training Loss: 0.6611 | Val Loss: 0.6635 | Accuracy: 0.6066 | Precision: 0.5799 | Recall: 0.6844\n",
            "Epoch 05 | Training Loss: 0.6625 | Val Loss: 0.6633 | Accuracy: 0.6004 | Precision: 0.6024 | Recall: 0.5169\n",
            "Epoch 06 | Training Loss: 0.6596 | Val Loss: 0.6615 | Accuracy: 0.6074 | Precision: 0.5905 | Recall: 0.6205\n",
            "Epoch 07 | Training Loss: 0.6577 | Val Loss: 0.6608 | Accuracy: 0.6074 | Precision: 0.5866 | Recall: 0.6444\n",
            "Epoch 08 | Training Loss: 0.6555 | Val Loss: 0.6624 | Accuracy: 0.6050 | Precision: 0.5765 | Recall: 0.6980\n",
            "Epoch 09 | Training Loss: 0.6544 | Val Loss: 0.6612 | Accuracy: 0.6072 | Precision: 0.5985 | Recall: 0.5763\n",
            "Epoch 10 | Training Loss: 0.6529 | Val Loss: 0.6618 | Accuracy: 0.6064 | Precision: 0.5792 | Recall: 0.6881\n",
            "\n",
            "Evaluating on test set...\n",
            "Test Loss: 0.6602 | Test Accuracy: 0.6082 | Test Precision: 0.5916 | Test Recall: 0.6990\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 2 Todos\n",
        "\n",
        "## Overview\n",
        "- **Objective:**  \n",
        "  Modify your model’s text preprocessing by changing from character-level tokenization to word-level tokenization. Compare the performance of both tokenization methods. Additionally, perform hyper-parameter optimization by experimenting with various settings (learning rate, hidden layers, hidden sizes, batch sizes, optimizers, and activation functions) and report your findings.\n",
        "\n",
        "## 1. Initial Setup\n",
        "- [ ] **Set Random Seeds:**  \n",
        "  Ensure reproducibility by setting seeds for all random number generators (e.g., Python’s `random`, NumPy, TensorFlow/PyTorch).\n",
        "  \n",
        "- [ ] **Prepare the Environment:**  \n",
        "  - Create a new or update an existing Jupyter Notebook.\n",
        "  - Ensure that all necessary libraries (e.g., NumPy, pandas, TensorFlow/PyTorch, matplotlib, etc.) are installed.\n",
        "  \n",
        "- [ ] **Version Control:**  \n",
        "  Initialize a Git repository (if not already done) and commit your initial setup.\n",
        "\n",
        "## 2. Data Preprocessing\n",
        "- [ ] **Load Dataset:**  \n",
        "  Load your dataset into the notebook.\n",
        "  \n",
        "- [ ] **Tokenization:**\n",
        "  - **Character-Level Tokenization:**  \n",
        "    - Tokenize the text data at the character level.\n",
        "    - Save and log the processed data.\n",
        "  - **Word-Level Tokenization:**  \n",
        "    - Modify the tokenization process to tokenize the text by words.\n",
        "    - Save and log the processed data.\n",
        "    \n",
        "- [ ] **Comparison:**  \n",
        "  - Create a section in your notebook to compare the two tokenization approaches.\n",
        "  - Visualize or tabulate differences in vocabulary size, sequence lengths, and other relevant metrics.\n",
        "\n",
        "## 3. Model Architecture\n",
        "- [ ] **Define the Model:**  \n",
        "  Develop a model (or models) that can handle both tokenization types. Include the following adjustable hyper-parameters:\n",
        "  - Learning rate\n",
        "  - Number of hidden layers\n",
        "  - Hidden sizes (neurons per layer)\n",
        "  - Batch sizes\n",
        "  - Optimizers (e.g., Adam, SGD, RMSProp)\n",
        "  - Activation functions (e.g., ReLU, Tanh, LeakyReLU)\n",
        "\n",
        "## 4. Hyper-Parameter Optimization\n",
        "- [ ] **Experiment Setup:**  \n",
        "  For each hyper-parameter configuration, perform at least 3 different tests to ensure robustness.\n",
        "  \n",
        "- [ ] **Grid/Random Search:**  \n",
        "  Set up a search over the following hyper-parameter ranges (example values provided):\n",
        "  - **Learning Rate:** `[0.001, 0.0005, 0.0001]`\n",
        "  - **Hidden Layers:** `[1, 2, 3]`\n",
        "  - **Hidden Sizes:** `[128, 256, 512]`\n",
        "  - **Batch Sizes:** `[32, 64, 128]`\n",
        "  - **Optimizers:** `[Adam, SGD, RMSProp]`\n",
        "  - **Activation Functions:** `[ReLU, Tanh, LeakyReLU]`\n",
        "  \n",
        "- [ ] **Logging:**  \n",
        "  Record the results (accuracy, loss, etc.) for each configuration in tables or charts.\n",
        "\n",
        "## 5. Model Training and Evaluation\n",
        "- [ ] **Training with Each Configuration:**  \n",
        "  Run experiments for both tokenization approaches with each set of hyper-parameters:\n",
        "  - Train the model at least 3 times per configuration (keeping the seed constant at this stage).\n",
        "  - Log training and validation performance.\n",
        "  \n",
        "- [ ] **Identify the Best Model:**  \n",
        "  Select the best performing configuration based on validation metrics (e.g., accuracy).\n",
        "\n",
        "## 6. Final Experiments\n",
        "- [ ] **Robustness Check:**  \n",
        "  Once the best model is identified:\n",
        "  - Re-run the experiments at least 3 times with different random seeds.\n",
        "  - Record the performance (accuracy) for each run.\n",
        "  \n",
        "- [ ] **Statistical Reporting:**  \n",
        "  - Compute the **mean accuracy** and **standard error** across these runs.\n",
        "  - Include these statistics in your report.\n",
        "\n",
        "## 7. Documentation and Reporting\n",
        "- [ ] **Jupyter Notebook:**  \n",
        "  - Ensure that your notebook is well-commented and clearly documents each step.\n",
        "  - Include code cells for setting seeds, data preprocessing, model building, training, evaluation, and visualization.\n",
        "  \n",
        "- [ ] **Detailed Report (Word Document):**  \n",
        "  Prepare a report that includes:\n",
        "  - **Introduction:** Objectives and overview of the work.\n",
        "  - **Methodology:** Detailed explanation of tokenization changes and hyper-parameter optimization strategy.\n",
        "  - **Experiments and Results:**  \n",
        "    - Comparison between character-level and word-level tokenization.\n",
        "    - Tables/graphs for hyper-parameter experiments.\n",
        "    - Final model performance with mean accuracy and standard error.\n",
        "  - **Discussion:** Analysis of results, challenges encountered, and insights.\n",
        "  - **Conclusion:** Summarize the key findings.\n",
        "  \n",
        "- [ ] **Submission:**  \n",
        "  - Submit your Jupyter Notebook.\n",
        "  - Submit your Word document report.\n",
        "  - Ensure that both files are included in your repository or submission package.\n",
        "\n",
        "## 8. Final Checklist\n",
        "- [ ] All experiments have at least 3 different tests.\n",
        "- [ ] Random seeds are set before any experiment.\n",
        "- [ ] Hyper-parameter optimization covers changes in learning rate, hidden layers, hidden sizes, batch sizes, optimizers, and activation functions.\n",
        "- [ ] The best model’s performance is verified with experiments on different seeds.\n",
        "- [ ] Best model should be compared with random model shown above.\n",
        "- [ ] The report clearly documents the methodology, experiments, results, and final conclusions.\n",
        "- [ ] If experiments are shown with deeper MLP_FA with best settings (Extra credits -- 2 points)\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:**  \n",
        "> Keep thorough logs and document any observations during your experiments. Clear documentation is key to reproducibility and understanding your results.\n",
        "\n"
      ],
      "metadata": {
        "id": "6j6mv098aASE"
      }
    }
  ]
}